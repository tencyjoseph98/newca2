{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:- Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the necessary libraries that are required for transformers model implementation is imported. Numpy and pandas are used for handling the data. sklearn is used to calculate the accuracy and split the data. Transformers are used to load the bert models. Torch is used for deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Checking the availability of the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU is highly recommended as it makes the training much faster. But in this project, CPU is used as dedicated GPU is not available in my laptop. Compared to GPU, CPU is much slower but still does its job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 :Loading and preprocessing the AG News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\", header=None)\n",
    "df.columns = [\"label\", \"title\", \"description\"]\n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"description\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step4 : A smaller dataset is used for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df.sample(5000)  # It will Reduce the dataset size for quick training\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sampled[\"text\"], df_sampled[\"label\"] - 1, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 : Tokenizing the function- converting text to numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 4. Tokenization Function\n",
    "def tokenize_function(examples, tokenizer, max_length=128):\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since transformers doesn't understand the text, its converted to numbers using the above function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6 : Automating Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Model Training and Evaluation\n",
    "def train_and_evaluate(model_name, batch_size, learning_rate):\n",
    "    print(f\"\\n🔹 Training Model: {model_name} | Batch Size: {batch_size} | Learning Rate: {learning_rate}\\n\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenization\n",
    "    train_encodings = tokenize_function(X_train.tolist(), tokenizer)\n",
    "    test_encodings = tokenize_function(X_test.tolist(), tokenizer)\n",
    "\n",
    "    # Converting the Data to PyTorch Dataset\n",
    "    class CustomDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "    train_dataset = CustomDataset(train_encodings, y_train.tolist())\n",
    "    test_dataset = CustomDataset(test_encodings, y_test.tolist())\n",
    "\n",
    "    # Loading the Model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(y_train))).to(device)\n",
    "\n",
    "    # Defining the Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{model_name.replace('/', '_')}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=1,  # Faster training\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # Training the Model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluating the Model\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=1)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "    print(f\"✅ Model: {model_name} | Batch Size: {batch_size} | Learning Rate: {learning_rate} | Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, A function is defined that will automatically train and test different transformer models with their respective hyperparameter settings. The function imports a tokenizer to convert the text into numerical format and pre-processes the data by tokenizing, padding, and truncating it for uniformity. It encapsulates the data into PyTorch-compatible dataset, then imports a pre-trained transformer model that we set to use GPU if available. Training arguments such as the batch size, learning rate, and number of epochs are set to maximize performance. The Hugging Face Trainer is used to manage the training process, and the model is evaluated on the test set once trained. Finally, the function calculates and prints the accuracy, allowing for the comparison of different models and hyperparameter settings to determine the best performing configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Running Experiments on the models and comparing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Training Model: prajjwal1/bert-tiny | Batch Size: 8 | Learning Rate: 2e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Tency Joseph\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.241000</td>\n",
       "      <td>1.112422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model: prajjwal1/bert-tiny | Batch Size: 8 | Learning Rate: 2e-05 | Accuracy: 0.7470\n",
      "\n",
      "🔹 Training Model: prajjwal1/bert-tiny | Batch Size: 16 | Learning Rate: 5e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Tency Joseph\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.957559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model: prajjwal1/bert-tiny | Batch Size: 16 | Learning Rate: 5e-05 | Accuracy: 0.8370\n",
      "\n",
      "🔹 Training Model: distilbert-base-uncased | Batch Size: 8 | Learning Rate: 2e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Tency Joseph\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 15:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.468900</td>\n",
       "      <td>0.303354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model: distilbert-base-uncased | Batch Size: 8 | Learning Rate: 2e-05 | Accuracy: 0.9070\n",
      "\n",
      "🔹 Training Model: distilbert-base-uncased | Batch Size: 16 | Learning Rate: 5e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Tency Joseph\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 12:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.276372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model: distilbert-base-uncased | Batch Size: 16 | Learning Rate: 5e-05 | Accuracy: 0.9080\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = {}\n",
    "\n",
    "# Model 1: TinyBERT (Fastest)\n",
    "results[\"bert-tiny_8_2e-5\"] = train_and_evaluate(\"prajjwal1/bert-tiny\", batch_size=8, learning_rate=2e-5)\n",
    "results[\"bert-tiny_16_5e-5\"] = train_and_evaluate(\"prajjwal1/bert-tiny\", batch_size=16, learning_rate=5e-5)\n",
    "\n",
    "# Model 2: DistilBERT (More Accurate)\n",
    "results[\"distilbert_8_2e-5\"] = train_and_evaluate(\"distilbert-base-uncased\", batch_size=8, learning_rate=2e-5)\n",
    "results[\"distilbert_16_5e-5\"] = train_and_evaluate(\"distilbert-base-uncased\", batch_size=16, learning_rate=5e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a comparison of BERT-Tiny and DistilBERT-Base-Uncased on the AG News dataset with different batch sizes and learning rates. BERT-Tiny achieved 74.7% accuracy with (batch size: 8, learning rate: 2e-5) and improved to 83.7% with (16, 5e-5). DistilBERT, being more efficient, outperformed BERT-Tiny, achieving 90.7% with (8, 2e-5) and 90.8% with (16, 5e-5). The minimal improvement in DistilBERT suggests that it generalizes well even with reduced batch sizes. Warning indicates that some model weights had been recently set, further proving the need to fine-tune. These observations highlight the performance impact of model choice and optimization of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Printing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 FINAL RESULTS COMPARISON\n",
      "bert-tiny_8_2e-5: Accuracy = 0.7470\n",
      "bert-tiny_16_5e-5: Accuracy = 0.8370\n",
      "distilbert_8_2e-5: Accuracy = 0.9070\n",
      "distilbert_16_5e-5: Accuracy = 0.9080\n"
     ]
    }
   ],
   "source": [
    "# ✅ 7. Print Final Comparison\n",
    "print(\"\\n🔹 FINAL RESULTS COMPARISON\")\n",
    "for config, acc in results.items():\n",
    "    print(f\"{config}: Accuracy = {acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Takeaways from Model Performance\n",
    "\n",
    "1)DistilBERT outperforms BERT-Tiny\n",
    "\n",
    "DistilBERT consistently gets a higher accuracy (~90.7-90.8%) than BERT-Tiny (~74.7-83.7%).\n",
    "\n",
    "This is to be expected since DistilBERT is designed to retain most of BERT's knowledge but is more efficient.\n",
    "\n",
    "2️)Increased Batch Size Enhances Stability\n",
    "\n",
    "Increasing the batch size from 8 to 16 helped both models, with BERT-Tiny improving considerably (74.7% → 83.7%).\n",
    "\n",
    "DistilBERT also saw a minor gain (90.7% → 90.8%), proving larger batches lead to smoother updates.\n",
    "\n",
    "However, very large batch sizes could cause overfitting (though not encountered here).\n",
    "\n",
    "3️)Higher Learning Rate Serves Small Models\n",
    "\n",
    "BERT-Tiny gained from increased learning rate (5e-5 performed better than 2e-5).\n",
    "\n",
    "Since it's a compact model, it can learn at a faster rate.\n",
    "\n",
    "DistilBERT, a more capable model, performed decently even on the lower learning rate.\n",
    "\n",
    "4)Best Overall Model: DistilBERT (Batch 16, LR 5e-5)\n",
    "\n",
    "It yielded the best accuracy (90.8%) along with faster training.\n",
    "\n",
    "And even when using batch 8 and LR 2e-5, DistilBERT still reached 90.7%, which is an evidence of its efficiency.\n",
    "\n",
    "Final Thought: Accuracy and training efficiency being the goal, the best model is DistilBERT with Batch 16 and LR 5e-5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
